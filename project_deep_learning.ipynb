{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       path_to_image Benign or Malignant  \\\n",
      "0  BreaKHis_v1/histology_slides/breast/benign/SOB...              Benign   \n",
      "1  BreaKHis_v1/histology_slides/breast/benign/SOB...              Benign   \n",
      "2  BreaKHis_v1/histology_slides/breast/benign/SOB...              Benign   \n",
      "3  BreaKHis_v1/histology_slides/breast/benign/SOB...              Benign   \n",
      "4  BreaKHis_v1/histology_slides/breast/benign/SOB...              Benign   \n",
      "\n",
      "  Cancer Type Magnification                      image_name  \n",
      "0    Adenosis          100X  SOB_B_A-14-22549AB-100-011.png  \n",
      "1    Adenosis          100X  SOB_B_A-14-22549AB-100-005.png  \n",
      "2    Adenosis          100X  SOB_B_A-14-22549AB-100-004.png  \n",
      "3    Adenosis          100X  SOB_B_A-14-22549AB-100-010.png  \n",
      "4    Adenosis          100X  SOB_B_A-14-22549AB-100-006.png  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"BreaKHis_v1/histology_slides/breast/image_data.csv\")\n",
    "\n",
    "# Extract image-related information from the path column if it exists\n",
    "if 'path_to_image' in df.columns:\n",
    "    df['image_name'] = df['path_to_image'].apply(lambda x: os.path.basename(x))\n",
    "    #df = df.drop(columns=['path_to_image'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    img = load_img(row['path_to_image'], target_size=(224, 224, 3))\n",
    "    img_array = img_to_array(img)  \n",
    "    image_data.append(img_array)\n",
    "\n",
    "df['image_data'] = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[[192.0, 180.0, 202.0], [174.0, 164.0, 198.0]...\n",
       "1    [[[179.0, 172.0, 179.0], [187.0, 152.0, 174.0]...\n",
       "2    [[[201.0, 211.0, 203.0], [198.0, 198.0, 200.0]...\n",
       "3    [[[206.0, 213.0, 206.0], [208.0, 213.0, 207.0]...\n",
       "4    [[[150.0, 145.0, 167.0], [156.0, 143.0, 173.0]...\n",
       "Name: image_data, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image_data'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for duplicated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary\n",
    "print(f\"Dataset contains {len(df)} images.\")\n",
    "\n",
    "# Calculate raw counts and percentages\n",
    "tumor_summary = df[\"Benign or Malignant\"].value_counts().reset_index()\n",
    "tumor_summary.columns = [\"Benign or Malignant\", \"Count\"]\n",
    "tumor_summary[\"Percentage (%)\"] = round((tumor_summary[\"Count\"] / len(df)) * 100, 2)\n",
    "\n",
    "# Summary of tumor classes and types\n",
    "print(\"\\nSummary of tumor classes:\")\n",
    "print(tumor_summary)\n",
    "\n",
    "print(\"\\nSummary of tumor types:\")\n",
    "print(df[[\"Benign or Malignant\",\"Cancer Type\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_data = df[df['Benign or Malignant'] == 'Benign']\n",
    "malignant_data = df[df['Benign or Malignant'] == 'Malignant']\n",
    "\n",
    "# Group for Benign tumors\n",
    "benign_grouped = benign_data.groupby('Magnification').size().reset_index(name='Count')\n",
    "\n",
    "# Group for Malignant tumors\n",
    "malignant_grouped = malignant_data.groupby('Magnification').size().reset_index(name='Count')\n",
    "\n",
    "# Plot for Benign tumors\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=benign_grouped, x='Magnification', y='Count', palette='viridis')\n",
    "plt.title('Benign Tumors by Magnification', fontsize=12)\n",
    "plt.xlabel('', fontsize=10)  # No label for x-axis\n",
    "plt.ylabel('', fontsize=10)  # No label for y-axis\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Malignant tumors\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=malignant_grouped, x='Magnification', y='Count', palette='viridis')\n",
    "plt.title('Malignant Tumors by Magnification', fontsize=12)\n",
    "plt.xlabel('', fontsize=10)  # No label for x-axis\n",
    "plt.ylabel('', fontsize=10)  # No label for y-axis\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable and Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['image_data'].tolist())  # Convert Series to a numpy array\n",
    "# Flatten X (from 3D to 2D)\n",
    "X = X.reshape(X.shape[0], -1)  # Flatten each image into a 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['Benign or Malignant'].apply(lambda x: 0 if x == \"Benign\" else 1)\n",
    "y = df['target'].values  # Assuming the target is in a column 'target'\n",
    "\n",
    "# Split the dataset into training and testing subsets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state = 1, shuffle = True)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.50, random_state = 1, stratify=y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42, shuffle = True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1\n",
    "##### 2 hidden layers with 16 hidden units each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with input_dim specified\n",
    "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=30, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the history object\n",
    "history_dict = history.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# Generate the range for epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary labels\n",
    "\n",
    "# Display some predicted probabilities and corresponding predictions\n",
    "print(\"\\nSample Predictions (Probability -> Binary):\")\n",
    "for i in range(10):  # Show the first 5 predictions\n",
    "    print(f\"Probability: {y_pred_prob[i][0]:.4f}, Predicted Class: {y_pred[i][0]}, Actual Class: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "##### 3 hidden layers with 16 hidden units each\n",
    "##### Tested 3 hidden layers with 32 units each, but the results were unsatisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sequential model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# First hidden layer with input_dim specified and more hidden units\n",
    "#model_2.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_2.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Second hidden layer with more hidden units\n",
    "#model_2.add(Dense(32, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Third hidden layer with more hidden units\n",
    "#model_2.add(Dense(32, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=20, batch_size=30, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the history object\n",
    "history_dict = history_2.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# Generate the range for epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "results = model_2.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_prob = model_2.predict(X_test)  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary labels\n",
    "\n",
    "# Display some predicted probabilities and corresponding predictions\n",
    "print(\"\\nSample Predictions (Probability -> Binary):\")\n",
    "for i in range(10):  # Show the first 5 predictions\n",
    "    print(f\"Probability: {y_pred_prob[i][0]:.4f}, Predicted Class: {y_pred[i][0]}, Actual Class: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3\n",
    "##### Equivalent to model 2 but with the loss function mse instead\n",
    "##### Also tried tanh activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sequential model\n",
    "model_3 = Sequential()\n",
    "\n",
    "# First hidden layer with input_dim specified and more hidden units\n",
    "#model_3.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_3.add(Dense(16, activation='tanh', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Second hidden layer with more hidden units\n",
    "#model_3.add(Dense(16, activation='relu'))\n",
    "model_3.add(Dense(16, activation='tanh'))\n",
    "\n",
    "# Third hidden layer with more hidden units\n",
    "#model_3.add(Dense(16, activation='relu'))\n",
    "model_3.add(Dense(16, activation='tanh'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=20, batch_size=30, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the history object\n",
    "history_dict = history_3.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# Generate the range for epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "results = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_prob = model_3.predict(X_test)  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary labels\n",
    "\n",
    "# Display some predicted probabilities and corresponding predictions\n",
    "print(\"\\nSample Predictions (Probability -> Binary):\")\n",
    "for i in range(10):  # Show the first 5 predictions\n",
    "    print(f\"Probability: {y_pred_prob[i][0]:.4f}, Predicted Class: {y_pred[i][0]}, Actual Class: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4\n",
    "##### CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['image_data'].tolist()) # Convert Series to a numpy array\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Normalize the image data (pixel values between 0 and 1)\n",
    "# X = X / 255.0  # Scale pixel values to [0, 1]\n",
    "\n",
    "df['target'] = df['Benign or Malignant'].apply(lambda x: 0 if x == \"Benign\" else 1)\n",
    "y = df['target'].values  # Assuming the target is in a column 'target'\n",
    "\n",
    "# Split the dataset into training (70%), testing (15%), and validation (15%) subsets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=1, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=1, shuffle=True)\n",
    "\n",
    "# Verify the shapes of the splits\n",
    "# print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "# print(f\"Validation data shape: {X_val.shape}, {y_val.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X = np.array(df['image_data'].tolist())  # Convert Series to a numpy array\n",
    "# Flatten X (from 3D to 2D)\n",
    "#X = X.reshape(X.shape[0], -1)  # Flatten each image into a 1D vector\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42, shuffle = True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp, shuffle=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Initialize ImageDataGenerator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "#     fill_mode='nearest'\n",
    "# )\n",
    "\n",
    "# # Load minority class images\n",
    "# benign_images = df[df['target'] == 0]['image_data'].values\n",
    "\n",
    "# # Generate exactly 500 augmented images\n",
    "# target_count = 500\n",
    "# augmented_images = []\n",
    "# for img in benign_images:\n",
    "#     img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels)\n",
    "#     for batch in datagen.flow(img, batch_size=1):\n",
    "#         augmented_images.append(batch[0])\n",
    "#         if len(augmented_images) >= target_count:\n",
    "#             break\n",
    "#     if len(augmented_images) >= target_count:\n",
    "#         break\n",
    "\n",
    "# # Convert augmented images to NumPy array\n",
    "# augmented_images = np.array(augmented_images)\n",
    "\n",
    "# # Flatten augmented images to match the shape of X_train\n",
    "# augmented_images_flattened = augmented_images.reshape(augmented_images.shape[0], -1)\n",
    "\n",
    "# # Add augmented data to training set\n",
    "# X_train = np.vstack((X_train, augmented_images_flattened))\n",
    "# y_train = np.hstack(([0] * len(augmented_images_flattened), y_train))\n",
    "\n",
    "# # Verify class balance\n",
    "# print(\"Updated class distribution:\")\n",
    "# print(pd.Series(y_train).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Use data augmentation on the minority class\n",
    "minority_class = np.where(y_train == 0)[0]\n",
    "minority_class_images = X_train[minority_class]\n",
    "minority_class_label = y_train[minority_class]\n",
    "\n",
    "# Generate augmented images\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "for i in range(500):\n",
    "    img = minority_class_images[i]\n",
    "    img = img.reshape((224, 224, 3))  # Reshape to 3D\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension to make it 4D\n",
    "    for _ in range(2):  # Generate 5 augmented images per original image\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            augmented_images.append(batch[0])\n",
    "            augmented_labels.append(minority_class_label[i])\n",
    "            break\n",
    "        \n",
    "# Convert lists to numpy arrays\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Reshape X_train back to 4D\n",
    "X_train_reshaped = X_train.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Combine augmented images with original images\n",
    "X_train_augmented = np.vstack((X_train_reshaped, augmented_images))\n",
    "y_train_augmented = np.hstack((y_train, augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4800\n",
      "0    1736\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train_augmented).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.5320 - accuracy: 0.8060"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 150528)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 32\u001b[0m\n\u001b[0;32m     27\u001b[0m model_4\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m     28\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Binary crossentropy is better for binary classification\u001b[39;00m\n\u001b[0;32m     29\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m history_4 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\DIOGOR~1\\AppData\\Local\\Temp\\__autograph_generated_file_u_hb6zs.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Diogo Reis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 150528)\n"
     ]
    }
   ],
   "source": [
    "X_val_reshaped = X_val.reshape(-1, 224, 224, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Build the CNN model\n",
    "model_4 = Sequential()\n",
    " \n",
    "# First convolutional layer\n",
    "model_4.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "# Second convolutional layer\n",
    "model_4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "# Third convolutional layer\n",
    "model_4.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "# Flatten the feature maps\n",
    "model_4.add(Flatten())\n",
    " \n",
    "# Fully connected layers\n",
    "model_4.add(Dense(64, activation='relu'))  # First dense layer\n",
    "model_4.add(Dense(16, activation='relu'))  # Second dense layer\n",
    " \n",
    "# Output layer for binary classification\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "# Compile the model\n",
    "model_4.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "                loss='binary_crossentropy',  # Binary crossentropy is better for binary classification\n",
    "                metrics=['accuracy'])\n",
    " \n",
    "# Train the model\n",
    "history_4 = model_4.fit(X_train_augmented, y_train_augmented, epochs=10, batch_size=30, validation_data=(X_val_reshaped, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_4.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary labels\n",
    "y_pred = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pre-trained models </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base layers (optional)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers for binary or multi-class classification\n",
    "model_ResNet50 = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "    # Use Dense(8, activation='softmax') for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_ResNet50.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy','precision','recall']) #binary\n",
    "#model_ResNet50.compile(optimizer=Adam(learning_rate=0.001), loss='crossentropy', metrics=['accuracy','precision','recall']) #multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ResNet50.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 without the top layer (pre-trained on ImageNet)\n",
    "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base layers (optional)\n",
    "base_model_vgg16.trainable = False\n",
    "\n",
    "# Add custom layers for binary or multi-class classification\n",
    "model_vgg16 = Sequential([\n",
    "    base_model_vgg16,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "    # Use Dense(8, activation='softmax') for multi-class classification (e.g., 8 cancer types)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_vgg16.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred_vgg16 = model_vgg16.predict(X_test)\n",
    "y_pred_vgg16 = np.round(y_pred_vgg16)\n",
    "print(classification_report(y_test, y_pred_vgg16))\n",
    "print(confusion_matrix(y_test, y_pred_vgg16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
